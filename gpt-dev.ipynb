{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"input.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    text = f.read(); \n",
    "\n",
    "# encoding: It translates characters to numbers\n",
    "# read the file to inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce85e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data set:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the data set: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0120d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d15cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#all the unique characters in the dataset: \n",
    "# create a hashmap -> turn it into a list -> sort it -> unique\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\" \".join(chars))\n",
    "print(vocab_size) # so there are 65 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n",
      "1  \n",
      "2 !\n",
      "3 $\n",
      "4 &\n",
      "5 '\n",
      "6 ,\n",
      "7 -\n",
      "8 .\n",
      "9 3\n",
      "10 :\n",
      "11 ;\n",
      "12 ?\n",
      "13 A\n",
      "14 B\n",
      "15 C\n",
      "16 D\n",
      "17 E\n",
      "18 F\n",
      "19 G\n",
      "20 H\n",
      "21 I\n",
      "22 J\n",
      "23 K\n",
      "24 L\n",
      "25 M\n",
      "26 N\n",
      "27 O\n",
      "28 P\n",
      "29 Q\n",
      "30 R\n",
      "31 S\n",
      "32 T\n",
      "33 U\n",
      "34 V\n",
      "35 W\n",
      "36 X\n",
      "37 Y\n",
      "38 Z\n",
      "39 a\n",
      "40 b\n",
      "41 c\n",
      "42 d\n",
      "43 e\n",
      "44 f\n",
      "45 g\n",
      "46 h\n",
      "47 i\n",
      "48 j\n",
      "49 k\n",
      "50 l\n",
      "51 m\n",
      "52 n\n",
      "53 o\n",
      "54 p\n",
      "55 q\n",
      "56 r\n",
      "57 s\n",
      "58 t\n",
      "59 u\n",
      "60 v\n",
      "61 w\n",
      "62 x\n",
      "63 y\n",
      "64 z\n"
     ]
    }
   ],
   "source": [
    "for i,ch in enumerate(chars): \n",
    "    print(i, ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e72079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n",
      " \n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Tokenization is the process of converting raw text into smaller units called tokens.\n",
    "# In the context of language models, a token can be a character, word, or subword.\n",
    "# For character-level models (like this one), each character is treated as a token.\n",
    "# creating a mapping from characters to integers\n",
    "\n",
    "# creates a dictionary called 'stoi' (string-to-integer), where each unique character in 'chars' is mapped to a unique integer index.\n",
    "# For example, if chars = ['a', 'b', 'c'], then stoi = {'a': 0, 'b': 1, 'c': 2}.\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# defines an encoder function that converts a string into a list of integers,\n",
    "# where each character in the string is mapped to its corresponding integer index using the 'stoi' dictionary.\n",
    "# For example, encode(\"abc\") would return [stoi['a'], stoi['b'], stoi['c']].\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))\n",
    "print(\" \")\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e0d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373]\n",
      "hello\n",
      " \n",
      "[71, 4178, 612]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# using libray to encode and decode\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\") # get the encoding for gpt2 model \n",
    "tokens = enc.encode(\"hello\")\n",
    "print(tokens)\n",
    "decoded = enc.decode(tokens)\n",
    "print(decoded)\n",
    "print(\" \")\n",
    "print(enc.encode(\"hii there\"))\n",
    "print(enc.decode(enc.encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52bc4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and sotre it into a torch tensor\n",
    "# A torch tensor is a multi-dimensional array, similar to a NumPy array, but with additional capabilities for GPU acceleration and automatic differentiation.\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long) # should be of int64 type; \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b95913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      " \n",
      "Validation data:\n",
      "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10,  0, 19, 53, 53, 42,  1, 51, 53,\n",
      "        56, 56, 53, 61,  6,  1, 52, 43, 47, 45, 46, 40, 53, 59, 56,  1, 14, 39,\n",
      "        54, 58, 47, 57, 58, 39,  8,  0,  0, 14, 13, 28, 32, 21, 31, 32, 13, 10,\n",
      "         0, 19, 53, 53, 42,  1, 51, 53, 56, 56, 53, 61,  6,  1, 52, 43, 47, 45,\n",
      "        46, 40, 53, 59, 56,  1, 19, 56, 43, 51, 47, 53,  8,  0, 19, 53, 42,  1,\n",
      "        57, 39, 60, 43,  1, 63, 53, 59,  6,  1])\n"
     ]
    }
   ],
   "source": [
    "# split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # 90% of the data for training, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"Train data:\")\n",
    "print(train_data[:100])\n",
    "print(\" \")\n",
    "print(\"Validation data:\")\n",
    "print(val_data[:100])\n",
    "\n",
    "# we want to create a dataset class that will handle the data loading and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bdb5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length \n",
    "first_chars = train_data[:block_size+1] # this is the first 9 characters of the training data\n",
    "# Convert tensor elements to Python ints before decoding\n",
    "print(first_chars)\n",
    "decoded_text = decode([int(idx) for idx in first_chars])\n",
    "print(decoded_text)\n",
    "\n",
    "# This is a list comprehension that iterates over each element 'idx' in the tensor 'first_chars'.\n",
    "# Each 'idx' is a PyTorch tensor scalar, so 'int(idx)' converts it to a standard Python integer.\n",
    "# The result is a list of Python integers corresponding to the character token IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18694911",
   "metadata": {},
   "source": [
    "- all these charcters follow each other when they follow each other\n",
    "- 8 individual examples pack together \n",
    "- if 18, 47 exist together, than 56 is very likely to come after and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40815518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+2]\n",
    "\n",
    "# y is offset by 1 from x because we want to predict the next character\n",
    "\n",
    "# size: 3 \n",
    "# x: [1,2,3,4] (size + 1)\n",
    "# y: [1,2,3,4,5] (size + 2)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  # This will reach up to but not including the last character when t = block_size - 1, since x has length block_size + 1\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "    \n",
    "    \n",
    "# this approach allow the model to get used to a single character to a hugh chunk from text\n",
    "# useful coz in inference, we want to be able to generate a text of any length (form one char to a huge chunk of text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3816960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dimensions: \n",
    "# many batches of multiple chunk of texts stacked up in a single sensor so that the model can process them in parallel\n",
    "# those chunks are being processed in parallel, but the model is still processing one character at a time\n",
    "torch.manual_seed(1337) # this is to make the random number generator deterministic\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "# the following function is used to generate a batch of data\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    # Select the appropriate dataset based on the split ('train' or 'val')\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Randomly choose 'batch_size' starting indices for the input sequences.\n",
    "    # Each index will be the start of a chunk of text of length 'block_size'.\n",
    "    \n",
    "    # Example: Suppose len(data) = 100 and block_size = 8, batch_size = 4\n",
    "    # torch.randint(100 - 8, (4,)) might return tensor([23, 56, 12, 70])\n",
    "    # These are the starting indices for each sequence in the batch.\n",
    "    \n",
    "    # (4,) is the shape of a 1-dimensional tensor (or numpy array) with 4 elements.\n",
    "    \n",
    "    # Why do we need to subtract 8? We subtract block_size (8) from the data length (100) to ensure that each randomly chosen starting index leaves enough room for a full sequence of length block_size. If we didn't subtract 8, some starting indices would go past the end of the data when we try to extract a chunk of length 8.\n",
    "    # so its shape is (4,). The comma indicates it's a tuple, and the single value means 1D.\n",
    "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # so this will return 4 random integers between 0 and 92 (100 - 8)\n",
    "\n",
    "    # For each random index, extract a sequence of length 'block_size' as input (x)\n",
    "    # and the next sequence (shifted by 1) as the target (y).\n",
    "    # This way, for each input sequence, the model learns to predict the next character at every position.\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
    "\n",
    "    # Example of how it works:\n",
    "    # Suppose:\n",
    "    #   - len(data) = 100\n",
    "    #   - block_size = 8\n",
    "    #   - batch_size = 4\n",
    "    # random_indices = tensor([23, 56, 12, 70])\n",
    "    # For each index:\n",
    "    #   - x[0] = data[23:31], y[0] = data[24:32]\n",
    "    #   - x[1] = data[56:64], y[1] = data[57:65]\n",
    "    #   - x[2] = data[12:20], y[2] = data[13:21]\n",
    "    #   - x[3] = data[70:78], y[3] = data[71:79]\n",
    "    # So, x is a (4, 8) tensor of input sequences, and y is a (4, 8) tensor of target sequences (the next character for each position).\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912794a5",
   "metadata": {},
   "source": [
    "![Explaining torch.stack method](images/Explaining-torch.stack-method-2048x1430.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c06dc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[50, 39, 52, 63,  1, 47, 58, 57],\n",
      "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
      "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 52, 63,  1, 47, 58, 57, 43],\n",
      "        [53, 63,  1, 42, 47, 42,  1, 57],\n",
      "        [51,  1, 39, 44, 56, 39, 47, 42],\n",
      "        [24, 21, 38, 13, 14, 17, 32, 20]])\n",
      "\n",
      "when input is tensor([50]) the target is 39\n",
      "when input is tensor([50, 39]) the target is 52\n",
      "when input is tensor([50, 39, 52]) the target is 63\n",
      "when input is tensor([50, 39, 52, 63]) the target is 1\n",
      "when input is tensor([50, 39, 52, 63,  1]) the target is 47\n",
      "when input is tensor([50, 39, 52, 63,  1, 47]) the target is 58\n",
      "when input is tensor([50, 39, 52, 63,  1, 47, 58]) the target is 57\n",
      "when input is tensor([50, 39, 52, 63,  1, 47, 58, 57]) the target is 43\n",
      "when input is tensor([56]) the target is 53\n",
      "when input is tensor([56, 53]) the target is 63\n",
      "when input is tensor([56, 53, 63]) the target is 1\n",
      "when input is tensor([56, 53, 63,  1]) the target is 42\n",
      "when input is tensor([56, 53, 63,  1, 42]) the target is 47\n",
      "when input is tensor([56, 53, 63,  1, 42, 47]) the target is 42\n",
      "when input is tensor([56, 53, 63,  1, 42, 47, 42]) the target is 1\n",
      "when input is tensor([56, 53, 63,  1, 42, 47, 42,  1]) the target is 57\n",
      "when input is tensor([39]) the target is 51\n",
      "when input is tensor([39, 51]) the target is 1\n",
      "when input is tensor([39, 51,  1]) the target is 39\n",
      "when input is tensor([39, 51,  1, 39]) the target is 44\n",
      "when input is tensor([39, 51,  1, 39, 44]) the target is 56\n",
      "when input is tensor([39, 51,  1, 39, 44, 56]) the target is 39\n",
      "when input is tensor([39, 51,  1, 39, 44, 56, 39]) the target is 47\n",
      "when input is tensor([39, 51,  1, 39, 44, 56, 39, 47]) the target is 42\n",
      "when input is tensor([17]) the target is 24\n",
      "when input is tensor([17, 24]) the target is 21\n",
      "when input is tensor([17, 24, 21]) the target is 38\n",
      "when input is tensor([17, 24, 21, 38]) the target is 13\n",
      "when input is tensor([17, 24, 21, 38, 13]) the target is 14\n",
      "when input is tensor([17, 24, 21, 38, 13, 14]) the target is 17\n",
      "when input is tensor([17, 24, 21, 38, 13, 14, 17]) the target is 32\n",
      "when input is tensor([17, 24, 21, 38, 13, 14, 17, 32]) the target is 20\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n",
    "# [57, 43, 60, 43, 52,  1, 63, 43] this is one of the batch of inputs\n",
    "# [43, 60, 43, 52,  1, 63, 43, 58] this is the corresponding batch of targets -> that will be used to calculate the loss after being passed to the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa2c2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50, 39, 52, 63,  1, 47, 58, 57],\n",
      "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
      "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model: \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337) # for the reproducibility of the results\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "    def __init__(self, vocab_size): \n",
    "        \n",
    "        super().__init__() # call the constructor of the parent class (nn.Module)\n",
    "        \n",
    "        # The following line creates an embedding table where each token in the vocabulary is mapped directly to a vector of size vocab_size.\n",
    "        # In this setup, the embedding table acts as a simple lookup table: for each input token, it outputs a vector (logits) that represents the scores for all possible next tokens.\n",
    "        # This means that the model is essentially learning, for each token, the probability distribution over the next token, without considering any context beyond the current token.\n",
    "        # thin wrapper around the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx is a tensor of shape (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx) # (batch_size, block_size, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1c082",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1422840216.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mToken embedding is a technique used in natural language processing to convert discrete tokens (such as words or characters) into continuous vector representations. Each unique token in the vocabulary is assigned a vector of real numbers, typically of fixed size. These vectors are learned during the training process and capture semantic or syntactic properties of the tokens.\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Token embedding is a technique used in natural language processing to convert discrete tokens (such as words or characters) into continuous vector representations. Each unique token in the vocabulary is assigned a vector of real numbers, typically of fixed size. These vectors are learned during the training process and capture semantic or syntactic properties of the tokens.\n",
    "\n",
    "In the context of neural networks, especially transformer-based models, token embeddings serve as the initial input to the model. Instead of processing raw token indices, the model works with their corresponding embeddings, which allows it to learn relationships and patterns in the data more effectively.\n",
    "\n",
    "For example, in PyTorch, `nn.Embedding(vocab_size, embedding_dim)` creates a lookup table where each token index maps to an embedding vector of size `embedding_dim`. In the BigramLanguageModel above, the embedding table is set up so that each token is mapped directly to a vector of size equal to the vocabulary size, which is then used to predict the next token.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
