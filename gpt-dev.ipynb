{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b31e63",
   "metadata": {},
   "source": [
    "- bigram model is a type of statistical language model that predicts the probability of a word or character based on the single preceding word or character. \n",
    "\n",
    "- Training Data: The model is trained on a large corpus of text. \n",
    "- Bigram Extraction: It identifies all consecutive pairs of words or characters (bigrams) within the training data. For example, in \"the cat sat,\" the bigrams are \"the cat\" and \"cat sat\". \n",
    "- Frequency Counting: The model counts how many times each bigram appears. \n",
    "Probability Calculation: The probability of a subsequent item is calculated by dividing the count of the specific bigram by the total count of the preceding item. For instance, the probability of \"cat\" following \"the\" would be the count of \"the cat\" divided by the count of \"the\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b3d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"input.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    text = f.read(); \n",
    "\n",
    "# encoding: It translates characters to numbers\n",
    "# read the file to inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce85e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data set:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the data set: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0120d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d15cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#all the unique characters in the dataset: \n",
    "# create a hashmap -> turn it into a list -> sort it -> unique\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\" \".join(chars))\n",
    "print(vocab_size) # so there are 65 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a9ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n",
      "1  \n",
      "2 !\n",
      "3 $\n",
      "4 &\n",
      "5 '\n",
      "6 ,\n",
      "7 -\n",
      "8 .\n",
      "9 3\n",
      "10 :\n",
      "11 ;\n",
      "12 ?\n",
      "13 A\n",
      "14 B\n",
      "15 C\n",
      "16 D\n",
      "17 E\n",
      "18 F\n",
      "19 G\n",
      "20 H\n",
      "21 I\n",
      "22 J\n",
      "23 K\n",
      "24 L\n",
      "25 M\n",
      "26 N\n",
      "27 O\n",
      "28 P\n",
      "29 Q\n",
      "30 R\n",
      "31 S\n",
      "32 T\n",
      "33 U\n",
      "34 V\n",
      "35 W\n",
      "36 X\n",
      "37 Y\n",
      "38 Z\n",
      "39 a\n",
      "40 b\n",
      "41 c\n",
      "42 d\n",
      "43 e\n",
      "44 f\n",
      "45 g\n",
      "46 h\n",
      "47 i\n",
      "48 j\n",
      "49 k\n",
      "50 l\n",
      "51 m\n",
      "52 n\n",
      "53 o\n",
      "54 p\n",
      "55 q\n",
      "56 r\n",
      "57 s\n",
      "58 t\n",
      "59 u\n",
      "60 v\n",
      "61 w\n",
      "62 x\n",
      "63 y\n",
      "64 z\n"
     ]
    }
   ],
   "source": [
    "for i,ch in enumerate(chars): \n",
    "    print(i, ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e72079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n",
      " \n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Tokenization is the process of converting raw text into smaller units called tokens.\n",
    "# In the context of language models, a token can be a character, word, or subword.\n",
    "# For character-level models (like this one), each character is treated as a token.\n",
    "# creating a mapping from characters to integers\n",
    "\n",
    "# creates a dictionary called 'stoi' (string-to-integer), where each unique character in 'chars' is mapped to a unique integer index.\n",
    "# For example, if chars = ['a', 'b', 'c'], then stoi = {'a': 0, 'b': 1, 'c': 2}.\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# defines an encoder function that converts a string into a list of integers,\n",
    "# where each character in the string is mapped to its corresponding integer index using the 'stoi' dictionary.\n",
    "# For example, encode(\"abc\") would return [stoi['a'], stoi['b'], stoi['c']].\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))\n",
    "print(\" \")\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e0d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373]\n",
      "hello\n",
      " \n",
      "[71, 4178, 612]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# using libray to encode and decode\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\") # get the encoding for gpt2 model \n",
    "tokens = enc.encode(\"hello\")\n",
    "print(tokens)\n",
    "decoded = enc.decode(tokens)\n",
    "print(decoded)\n",
    "print(\" \")\n",
    "print(enc.encode(\"hii there\"))\n",
    "print(enc.decode(enc.encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52bc4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and sotre it into a torch tensor\n",
    "# A torch tensor is a multi-dimensional array, similar to a NumPy array, but with additional capabilities for GPU acceleration and automatic differentiation.\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long) # should be of int64 type; \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b95913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      " \n",
      "Validation data:\n",
      "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10,  0, 19, 53, 53, 42,  1, 51, 53,\n",
      "        56, 56, 53, 61,  6,  1, 52, 43, 47, 45, 46, 40, 53, 59, 56,  1, 14, 39,\n",
      "        54, 58, 47, 57, 58, 39,  8,  0,  0, 14, 13, 28, 32, 21, 31, 32, 13, 10,\n",
      "         0, 19, 53, 53, 42,  1, 51, 53, 56, 56, 53, 61,  6,  1, 52, 43, 47, 45,\n",
      "        46, 40, 53, 59, 56,  1, 19, 56, 43, 51, 47, 53,  8,  0, 19, 53, 42,  1,\n",
      "        57, 39, 60, 43,  1, 63, 53, 59,  6,  1])\n"
     ]
    }
   ],
   "source": [
    "# split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # 90% of the data for training, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"Train data:\")\n",
    "print(train_data[:100])\n",
    "print(\" \")\n",
    "print(\"Validation data:\")\n",
    "print(val_data[:100])\n",
    "\n",
    "# we want to create a dataset class that will handle the data loading and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdb5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length \n",
    "first_chars = train_data[:block_size+1] # this is the first 9 characters of the training data\n",
    "# Convert tensor elements to Python ints before decoding\n",
    "print(first_chars)\n",
    "decoded_text = decode([int(idx) for idx in first_chars])\n",
    "print(decoded_text)\n",
    "\n",
    "# This is a list comprehension that iterates over each element 'idx' in the tensor 'first_chars'.\n",
    "# Each 'idx' is a PyTorch tensor scalar, so 'int(idx)' converts it to a standard Python integer.\n",
    "# The result is a list of Python integers corresponding to the character token IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18694911",
   "metadata": {},
   "source": [
    "- all these charcters follow each other when they follow each other\n",
    "- 8 individual examples pack together \n",
    "- if 18, 47 exist together, than 56 is very likely to come after and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40815518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+2]\n",
    "\n",
    "# y is offset by 1 from x because we want to predict the next character\n",
    "\n",
    "# size: 3 \n",
    "# x: [1,2,3,4] (size + 1)\n",
    "# y: [1,2,3,4,5] (size + 2)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  # This will reach up to but not including the last character when t = block_size - 1, since x has length block_size + 1\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "    \n",
    "    \n",
    "# this approach allow the model to get used to a single character to a hugh chunk from text\n",
    "# useful coz in inference, we want to be able to generate a text of any length (form one char to a huge chunk of text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3816960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dimensions: \n",
    "# many batches of multiple chunk of texts stacked up in a single sensor so that the model can process them in parallel\n",
    "# those chunks are being processed in parallel, but the model is still processing one character at a time\n",
    "torch.manual_seed(1337) # this is to make the random number generator deterministic\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "# the following function is used to generate a batch of data\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    # Select the appropriate dataset based on the split ('train' or 'val')\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Randomly choose 'batch_size' starting indices for the input sequences.\n",
    "    # Each index will be the start of a chunk of text of length 'block_size'.\n",
    "    \n",
    "    # Example: Suppose len(data) = 100 and block_size = 8, batch_size = 4\n",
    "    # torch.randint(100 - 8, (4,)) might return tensor([23, 56, 12, 70])\n",
    "    # These are the starting indices for each sequence in the batch.\n",
    "    \n",
    "    # (4,) is the shape of a 1-dimensional tensor (or numpy array) with 4 elements.\n",
    "    \n",
    "    # Why do we need to subtract 8? We subtract block_size (8) from the data length (100) to ensure that each randomly chosen starting index leaves enough room for a full sequence of length block_size. If we didn't subtract 8, some starting indices would go past the end of the data when we try to extract a chunk of length 8.\n",
    "    # so its shape is (4,). The comma indicates it's a tuple, and the single value means 1D.\n",
    "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # so this will return 4 random integers between 0 and 92 (100 - 8)\n",
    "\n",
    "    # For each random index, extract a sequence of length 'block_size' as input (x)\n",
    "    # and the next sequence (shifted by 1) as the target (y).\n",
    "    # This way, for each input sequence, the model learns to predict the next character at every position.\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
    "\n",
    "    # Example of how it works:\n",
    "    # Suppose:\n",
    "    #   - len(data) = 100\n",
    "    #   - block_size = 8\n",
    "    #   - batch_size = 4\n",
    "    # Why is this (4,) but the stack below is (8,)? \n",
    "    # random_indices = tensor([23, 56, 12, 70])  # Shape: (4,) -- one start index per batch\n",
    "    # When we stack slices of length block_size (which is 8), each slice is (8,), so stacking 4 of them gives (4, 8)\n",
    "    # For each index:\n",
    "    # Example:\n",
    "    #   - x[0] = data[23:31], y[0] = data[24:32]\n",
    "    #   - x[1] = data[56:64], y[1] = data[57:65]\n",
    "    #   - x[2] = data[12:20], y[2] = data[13:21]\n",
    "    #   - x[3] = data[70:78], y[3] = data[71:79]\n",
    "    \n",
    "    \n",
    "    # [23:31] means a slice of the data tensor, starting at index 23 (inclusive) and ending at index 31 (exclusive).\n",
    "    # In Python, data[23:31] returns the elements at positions 23, 24, ..., 30 (but not 31).\n",
    "    # used to extract a sequence of length 8 (since 31 - 23 = 8) from the data.\n",
    "    # In other words, for each batch, x is a chunk of 8 items, and y is the same chunk but shifted by 1.\n",
    "    # So, x is a (4, 8) tensor of input sequences, and y is a (4, 8) tensor of target sequences (the next character for each position).\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912794a5",
   "metadata": {},
   "source": [
    "![Explaining torch.stack method](images/Explaining-torch.stack-method-2048x1430.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c06dc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "when input is tensor([24]) the target is 43\n",
      "when input is tensor([24, 43]) the target is 58\n",
      "when input is tensor([24, 43, 58]) the target is 5\n",
      "when input is tensor([24, 43, 58,  5]) the target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "when input is tensor([44]) the target is 53\n",
      "when input is tensor([44, 53]) the target is 56\n",
      "when input is tensor([44, 53, 56]) the target is 1\n",
      "when input is tensor([44, 53, 56,  1]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52]) the target is 58\n",
      "when input is tensor([52, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58]) the target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "when input is tensor([25]) the target is 17\n",
      "when input is tensor([25, 17]) the target is 27\n",
      "when input is tensor([25, 17, 27]) the target is 10\n",
      "when input is tensor([25, 17, 27, 10]) the target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n",
    "# [57, 43, 60, 43, 52,  1, 63, 43] this is one of the batch of inputs\n",
    "# [43, 60, 43, 52,  1, 63, 43, 58] this is the corresponding batch of targets -> that will be used to calculate the loss after being passed to the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2c2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)\n",
    "# get four stacks of input, each stack of (8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e1d1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model: \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337) # for the reproducibility of the results\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "    def __init__(self, vocab_size): \n",
    "        \n",
    "        super().__init__() # call the constructor of the parent class (nn.Module)\n",
    "        \n",
    "        # creates an embedding table where each token in the vocabulary is mapped directly to a vector of size vocab_size.\n",
    "        # In this setup, the embedding table acts as a simple lookup table: for each input token, it outputs a vector (logits) that represents the scores for all possible next tokens.\n",
    "        # This means that the model is essentially learning, for each token, the probability distribution over the next token, without considering any context beyond the current token.\n",
    "        # thin wrapper around the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is a tensor of shape (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx) # (batch_size, time, channel)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            # stretching out the array, and add C as the second dimension: \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entropy loss -> measure the quality of the model's predictions with respect to the true targets (how well are we predicting the next character)\n",
    "            #only the target hsould have a reallly high dimension, the rest should be 0 or close to zero\n",
    "            \n",
    "        return logits, loss\n",
    "    # logits is the score for the next character in the sequence \n",
    "    # predicting what come next based on the current character \n",
    "    \n",
    "    \n",
    "    # The following code defines a method to generate new tokens (for example, text) from the model, given an initial sequence of token indices.\n",
    "    # - The method takes in `idx`, which is a tensor containing the current context (shape: batch size B, sequence length T).\n",
    "    # - For a specified number of steps (`max_new_tokens`), it repeatedly:\n",
    "    #   1. Feeds the current sequence to the model to get predictions (logits).\n",
    "    #   2. Focuses on the logits(scores) for the last token in the sequence, which represent the model's prediction for the next token.\n",
    "    #   3. Applies a softmax to convert logits into probabilities.\n",
    "    #   4. Samples the next token index from this probability distribution.\n",
    "    #   5. Appends the sampled token to the sequence, extending the context.\n",
    "    # - After generating the desired number of new tokens, it returns the full sequence including the newly generated tokens.\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # idx is the current context, shape (B,T)\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # get the logits for the last time step\n",
    "            logits = logits[:, -1, :] # (B,C): get predictions for the last time step\n",
    "            # last element in the time dimension and then stretch it out to (B,C)\n",
    "            \n",
    "            # convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # convert logits to probabilities\n",
    "            \n",
    "            # sample next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample next token\n",
    "            \n",
    "            # append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=-1) # append to sequence (b, t+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21b1a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logit, loss = m(xb, yb)\n",
    "print(logit.shape)\n",
    "print(loss)   \n",
    "\n",
    "\n",
    "# we are expecting a loss around -ln(1/65) = 4.17\n",
    "# that is using the uniform distribution to predict the next character\n",
    "# a value higher than that implies that the model is not diffuse and has a bit of entropy, such that it failed to learn the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aba1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # B, T = 1,1 holding zeros\n",
    "# 0 is standing as the new line character -> feed in as the first character\n",
    "\n",
    "# generates 100 new tokens from the model, starting with an initial context of a single token (the newline character, represented by 0).\n",
    "# The model's generate method returns a tensor of token indices, which is then converted to a list.\n",
    "# The decode function translates these token indices back into readable text, which is printed out.\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) \n",
    "\n",
    "# the code generated doesnt make sense because: \n",
    "# assuming we want to predict the character at position t, we only need the character at position t-1\n",
    "# but we feed in the whole context of the text so far, and use the last character as the input\n",
    "# history is not used rn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33a1c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToken embedding is a technique used in natural language processing to convert discrete tokens (such as words or characters) into continuous vector representations. Each unique token in the vocabulary is assigned a vector of real numbers, typically of fixed size. These vectors are learned during the training process and capture semantic or syntactic properties of the tokens.\\nIn the context of neural networks, especially transformer-based models, token embeddings serve as the initial input to the model. Instead of processing raw token indices, the model works with their corresponding embeddings, which allows it to learn relationships and patterns in the data more effectively.\\nFor example, in PyTorch, `nn.Embedding(vocab_size, embedding_dim)` creates a lookup table where each token index maps to an embedding vector of size `embedding_dim`. In the BigramLanguageModel above, the embedding table is set up so that each token is mapped directly to a vector of size equal to the vocabulary size, which is then used to predict the next token.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Token embedding is a technique used in natural language processing to convert discrete tokens (such as words or characters) into continuous vector representations. Each unique token in the vocabulary is assigned a vector of real numbers, typically of fixed size. These vectors are learned during the training process and capture semantic or syntactic properties of the tokens.\n",
    "In the context of neural networks, especially transformer-based models, token embeddings serve as the initial input to the model. Instead of processing raw token indices, the model works with their corresponding embeddings, which allows it to learn relationships and patterns in the data more effectively.\n",
    "For example, in PyTorch, `nn.Embedding(vocab_size, embedding_dim)` creates a lookup table where each token index maps to an embedding vector of size `embedding_dim`. In the BigramLanguageModel above, the embedding table is set up so that each token is mapped directly to a vector of size equal to the vocabulary size, which is then used to predict the next token.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57b63c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimization object\n",
    "# simpler model torch.optim.SGD(m.parameters(), lr=1e-3)\n",
    "# typical learnign rate: 3e-4, 1e-3, 3e-2\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0bc980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "# This section of code trains the language model using mini-batch gradient descent.\n",
    "# It sets the batch size, then runs a training loop for 10,000 steps.\n",
    "# In each step:\n",
    "#   - It samples a batch of training data (input and target sequences).\n",
    "#   - It computes the model's predictions and the loss.\n",
    "#   - It clears previous gradients, performs backpropagation to compute new gradients, and updates the model's parameters using the optimizer.\n",
    "# After training, it prints the final loss value.\n",
    "\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # backpropagate the loss\n",
    "    # zero_grad means that the gradients are set to zero, and set_to_none=True means that the gradients are not stored\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1937248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))\n",
    "# doing the same thing as before but now the model is trained, if we sample from it again, it will generate coherent text (not best pereformance, but a lot better)\n",
    "# the model is not communicating with the rest of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f8020",
   "metadata": {},
   "source": [
    "The mathematical trick in self-attention: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df388ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.3596, -0.9152],\n",
      "         [ 0.6258,  0.0255],\n",
      "         [ 0.9545,  0.0643],\n",
      "         [ 0.3612,  1.1679],\n",
      "         [-1.3499, -0.5102],\n",
      "         [ 0.2360, -0.2398],\n",
      "         [-0.9211,  1.5433]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.2858,  0.9651],\n",
      "         [-2.0371,  0.4931],\n",
      "         [ 1.4870,  0.5910],\n",
      "         [ 0.1260, -1.5627],\n",
      "         [-1.1601, -0.3348],\n",
      "         [ 0.4478, -0.8016],\n",
      "         [ 1.5236,  2.5086]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 1.0101,  0.1215],\n",
      "         [ 0.1584,  1.1340],\n",
      "         [-1.1539, -0.2984],\n",
      "         [-0.5075, -0.9239],\n",
      "         [ 0.5467, -1.4948],\n",
      "         [-1.2057,  0.5718],\n",
      "         [-0.5974, -0.6937]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.3514, -0.2759],\n",
      "         [-1.5108,  2.1048],\n",
      "         [ 2.7630, -1.7465],\n",
      "         [ 1.4516, -1.5103],\n",
      "         [ 0.8212, -0.2115],\n",
      "         [ 0.7789,  1.5333],\n",
      "         [ 1.6097, -0.4032]]])\n"
     ]
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # Batch, time, channel\n",
    "x = torch.randn(B,T,C)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "# this is a 4x8 tensor, each element is either 0 or 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
