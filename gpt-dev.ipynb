{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b31e63",
   "metadata": {},
   "source": [
    "- bigram model is a type of statistical language model that predicts the probability of a word or character based on the single preceding word or character. \n",
    "\n",
    "- Training Data: The model is trained on a large corpus of text. \n",
    "- Bigram Extraction: It identifies all consecutive pairs of words or characters (bigrams) within the training data. For example, in \"the cat sat,\" the bigrams are \"the cat\" and \"cat sat\". \n",
    "- Frequency Counting: The model counts how many times each bigram appears. \n",
    "Probability Calculation: The probability of a subsequent item is calculated by dividing the count of the specific bigram by the total count of the preceding item. For instance, the probability of \"cat\" following \"the\" would be the count of \"the cat\" divided by the count of \"the\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d2b3d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"input.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    text = f.read(); \n",
    "\n",
    "# encoding: It translates characters to numbers\n",
    "# read the file to inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0ce85e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the data set:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the data set: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0120d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d3d15cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#all the unique characters in the dataset: \n",
    "# create a hashmap -> turn it into a list -> sort it -> unique\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\" \".join(chars))\n",
    "print(vocab_size) # so there are 65 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "17a9ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\n",
      "1  \n",
      "2 !\n",
      "3 $\n",
      "4 &\n",
      "5 '\n",
      "6 ,\n",
      "7 -\n",
      "8 .\n",
      "9 3\n",
      "10 :\n",
      "11 ;\n",
      "12 ?\n",
      "13 A\n",
      "14 B\n",
      "15 C\n",
      "16 D\n",
      "17 E\n",
      "18 F\n",
      "19 G\n",
      "20 H\n",
      "21 I\n",
      "22 J\n",
      "23 K\n",
      "24 L\n",
      "25 M\n",
      "26 N\n",
      "27 O\n",
      "28 P\n",
      "29 Q\n",
      "30 R\n",
      "31 S\n",
      "32 T\n",
      "33 U\n",
      "34 V\n",
      "35 W\n",
      "36 X\n",
      "37 Y\n",
      "38 Z\n",
      "39 a\n",
      "40 b\n",
      "41 c\n",
      "42 d\n",
      "43 e\n",
      "44 f\n",
      "45 g\n",
      "46 h\n",
      "47 i\n",
      "48 j\n",
      "49 k\n",
      "50 l\n",
      "51 m\n",
      "52 n\n",
      "53 o\n",
      "54 p\n",
      "55 q\n",
      "56 r\n",
      "57 s\n",
      "58 t\n",
      "59 u\n",
      "60 v\n",
      "61 w\n",
      "62 x\n",
      "63 y\n",
      "64 z\n"
     ]
    }
   ],
   "source": [
    "for i,ch in enumerate(chars): \n",
    "    print(i, ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "70e72079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n",
      " \n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Tokenization is the process of converting raw text into smaller units called tokens.\n",
    "# In the context of language models, a token can be a character, word, or subword.\n",
    "# For character-level models (like this one), each character is treated as a token.\n",
    "# creating a mapping from characters to integers\n",
    "\n",
    "# creates a dictionary called 'stoi' (string-to-integer), where each unique character in 'chars' is mapped to a unique integer index.\n",
    "# For example, if chars = ['a', 'b', 'c'], then stoi = {'a': 0, 'b': 1, 'c': 2}.\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# defines an encoder function that converts a string into a list of integers,\n",
    "# where each character in the string is mapped to its corresponding integer index using the 'stoi' dictionary.\n",
    "# For example, encode(\"abc\") would return [stoi['a'], stoi['b'], stoi['c']].\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hello\"))\n",
    "print(decode(encode(\"hello\")))\n",
    "print(\" \")\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "30e0d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373]\n",
      "hello\n",
      " \n",
      "[71, 4178, 612]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# using libray to encode and decode\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\") # get the encoding for gpt2 model \n",
    "tokens = enc.encode(\"hello\")\n",
    "print(tokens)\n",
    "decoded = enc.decode(tokens)\n",
    "print(decoded)\n",
    "print(\" \")\n",
    "print(enc.encode(\"hii there\"))\n",
    "print(enc.decode(enc.encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e52bc4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and sotre it into a torch tensor\n",
    "# A torch tensor is a multi-dimensional array, similar to a NumPy array, but with additional capabilities for GPU acceleration and automatic differentiation.\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long) # should be of int64 type; \n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "75b95913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      " \n",
      "Validation data:\n",
      "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10,  0, 19, 53, 53, 42,  1, 51, 53,\n",
      "        56, 56, 53, 61,  6,  1, 52, 43, 47, 45, 46, 40, 53, 59, 56,  1, 14, 39,\n",
      "        54, 58, 47, 57, 58, 39,  8,  0,  0, 14, 13, 28, 32, 21, 31, 32, 13, 10,\n",
      "         0, 19, 53, 53, 42,  1, 51, 53, 56, 56, 53, 61,  6,  1, 52, 43, 47, 45,\n",
      "        46, 40, 53, 59, 56,  1, 19, 56, 43, 51, 47, 53,  8,  0, 19, 53, 42,  1,\n",
      "        57, 39, 60, 43,  1, 63, 53, 59,  6,  1])\n"
     ]
    }
   ],
   "source": [
    "# split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # 90% of the data for training, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"Train data:\")\n",
    "print(train_data[:100])\n",
    "print(\" \")\n",
    "print(\"Validation data:\")\n",
    "print(val_data[:100])\n",
    "\n",
    "# we want to create a dataset class that will handle the data loading and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1bdb5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "block_size = 8 # context length \n",
    "first_chars = train_data[:block_size+1] # this is the first 9 characters of the training data\n",
    "# Convert tensor elements to Python ints before decoding\n",
    "print(first_chars)\n",
    "decoded_text = decode([int(idx) for idx in first_chars])\n",
    "print(decoded_text)\n",
    "\n",
    "# This is a list comprehension that iterates over each element 'idx' in the tensor 'first_chars'.\n",
    "# Each 'idx' is a PyTorch tensor scalar, so 'int(idx)' converts it to a standard Python integer.\n",
    "# The result is a list of Python integers corresponding to the character token IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18694911",
   "metadata": {},
   "source": [
    "- all these charcters follow each other when they follow each other\n",
    "- 8 individual examples pack together \n",
    "- if 18, 47 exist together, than 56 is very likely to come after and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "40815518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+2]\n",
    "\n",
    "# y is offset by 1 from x because we want to predict the next character\n",
    "\n",
    "# size: 3 \n",
    "# x: [1,2,3,4] (size + 1)\n",
    "# y: [1,2,3,4,5] (size + 2)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  # This will reach up to but not including the last character when t = block_size - 1, since x has length block_size + 1\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")\n",
    "    \n",
    "    \n",
    "# this approach allow the model to get used to a single character to a hugh chunk from text\n",
    "# useful coz in inference, we want to be able to generate a text of any length (form one char to a huge chunk of text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b3816960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dimensions: \n",
    "# many batches of multiple chunk of texts stacked up in a single sensor so that the model can process them in parallel\n",
    "# those chunks are being processed in parallel, but the model is still processing one character at a time\n",
    "torch.manual_seed(1337) # this is to make the random number generator deterministic\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "# the following function is used to generate a batch of data\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    # Select the appropriate dataset based on the split ('train' or 'val')\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Randomly choose 'batch_size' starting indices for the input sequences.\n",
    "    # Each index will be the start of a chunk of text of length 'block_size'.\n",
    "    \n",
    "    # Example: Suppose len(data) = 100 and block_size = 8, batch_size = 4\n",
    "    # torch.randint(100 - 8, (4,)) might return tensor([23, 56, 12, 70])\n",
    "    # These are the starting indices for each sequence in the batch.\n",
    "    \n",
    "    # (4,) is the shape of a 1-dimensional tensor (or numpy array) with 4 elements.\n",
    "    \n",
    "    # Why do we need to subtract 8? We subtract block_size (8) from the data length (100) to ensure that each randomly chosen starting index leaves enough room for a full sequence of length block_size. If we didn't subtract 8, some starting indices would go past the end of the data when we try to extract a chunk of length 8.\n",
    "    # so its shape is (4,). The comma indicates it's a tuple, and the single value means 1D.\n",
    "    random_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # so this will return 4 random integers between 0 and 92 (100 - 8)\n",
    "\n",
    "    # For each random index, extract a sequence of length 'block_size' as input (x)\n",
    "    # and the next sequence (shifted by 1) as the target (y).\n",
    "    # This way, for each input sequence, the model learns to predict the next character at every position.\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in random_indices])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in random_indices])\n",
    "\n",
    "    # Example of how it works:\n",
    "    # Suppose:\n",
    "    #   - len(data) = 100\n",
    "    #   - block_size = 8\n",
    "    #   - batch_size = 4\n",
    "    # Why is this (4,) but the stack below is (8,)? \n",
    "    # random_indices = tensor([23, 56, 12, 70])  # Shape: (4,) -- one start index per batch\n",
    "    # When we stack slices of length block_size (which is 8), each slice is (8,), so stacking 4 of them gives (4, 8)\n",
    "    # For each index:\n",
    "    # Example:\n",
    "    #   - x[0] = data[23:31], y[0] = data[24:32]\n",
    "    #   - x[1] = data[56:64], y[1] = data[57:65]\n",
    "    #   - x[2] = data[12:20], y[2] = data[13:21]\n",
    "    #   - x[3] = data[70:78], y[3] = data[71:79]\n",
    "    \n",
    "    \n",
    "    # [23:31] means a slice of the data tensor, starting at index 23 (inclusive) and ending at index 31 (exclusive).\n",
    "    # In Python, data[23:31] returns the elements at positions 23, 24, ..., 30 (but not 31).\n",
    "    # used to extract a sequence of length 8 (since 31 - 23 = 8) from the data.\n",
    "    # In other words, for each batch, x is a chunk of 8 items, and y is the same chunk but shifted by 1.\n",
    "    # So, x is a (4, 8) tensor of input sequences, and y is a (4, 8) tensor of target sequences (the next character for each position).\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912794a5",
   "metadata": {},
   "source": [
    "![Explaining torch.stack method](images/Explaining-torch.stack-method-2048x1430.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c06dc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "when input is tensor([24]) the target is 43\n",
      "when input is tensor([24, 43]) the target is 58\n",
      "when input is tensor([24, 43, 58]) the target is 5\n",
      "when input is tensor([24, 43, 58,  5]) the target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "when input is tensor([44]) the target is 53\n",
      "when input is tensor([44, 53]) the target is 56\n",
      "when input is tensor([44, 53, 56]) the target is 1\n",
      "when input is tensor([44, 53, 56,  1]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52]) the target is 58\n",
      "when input is tensor([52, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58]) the target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "when input is tensor([25]) the target is 17\n",
      "when input is tensor([25, 17]) the target is 27\n",
      "when input is tensor([25, 17, 27]) the target is 10\n",
      "when input is tensor([25, 17, 27, 10]) the target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"\")\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n",
    "# [57, 43, 60, 43, 52,  1, 63, 43] this is one of the batch of inputs\n",
    "# [43, 60, 43, 52,  1, 63, 43, 58] this is the corresponding batch of targets -> that will be used to calculate the loss after being passed to the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fa2c2ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)\n",
    "# get four stacks of input, each stack of (8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6e1d1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram language model: \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337) # for the reproducibility of the results\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module): # subclass of nn.Module\n",
    "    def __init__(self, vocab_size): \n",
    "        \n",
    "        super().__init__() # call the constructor of the parent class (nn.Module)\n",
    "        \n",
    "        # creates an embedding table where each token in the vocabulary is mapped directly to a vector of size vocab_size.\n",
    "        # In this setup, the embedding table acts as a simple lookup table: for each input token, it outputs a vector (logits) that represents the scores for all possible next tokens.\n",
    "        # This means that the model is essentially learning, for each token, the probability distribution over the next token, without considering any context beyond the current token.\n",
    "        # thin wrapper around the lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is a tensor of shape (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx) # (batch_size, time, channel)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            # stretching out the array, and add C as the second dimension: \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross entropy loss -> measure the quality of the model's predictions with respect to the true targets (how well are we predicting the next character)\n",
    "            #only the target hsould have a reallly high dimension, the rest should be 0 or close to zero\n",
    "            \n",
    "        return logits, loss\n",
    "    # logits is the score for the next character in the sequence \n",
    "    # predicting what come next based on the current character \n",
    "    \n",
    "    \n",
    "    # The following code defines a method to generate new tokens (for example, text) from the model, given an initial sequence of token indices.\n",
    "    # - The method takes in `idx`, which is a tensor containing the current context (shape: batch size B, sequence length T).\n",
    "    # - For a specified number of steps (`max_new_tokens`), it repeatedly:\n",
    "    #   1. Feeds the current sequence to the model to get predictions (logits).\n",
    "    #   2. Focuses on the logits(scores) for the last token in the sequence, which represent the model's prediction for the next token.\n",
    "    #   3. Applies a softmax to convert logits into probabilities.\n",
    "    #   4. Samples the next token index from this probability distribution.\n",
    "    #   5. Appends the sampled token to the sequence, extending the context.\n",
    "    # - After generating the desired number of new tokens, it returns the full sequence including the newly generated tokens.\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # idx is the current context, shape (B,T)\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # get the logits for the last time step\n",
    "            logits = logits[:, -1, :] # (B,C): get predictions for the last time step\n",
    "            # last element in the time dimension and then stretch it out to (B,C)\n",
    "            \n",
    "            # convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # convert logits to probabilities\n",
    "            \n",
    "            # sample next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # sample next token\n",
    "            \n",
    "            # append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=-1) # append to sequence (b, t+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d21b1a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logit, loss = m(xb, yb)\n",
    "print(logit.shape)\n",
    "print(loss)   \n",
    "\n",
    "\n",
    "# we are expecting a loss around -ln(1/65) = 4.17\n",
    "# that is using the uniform distribution to predict the next character\n",
    "# a value higher than that implies that the model is not diffuse and has a bit of entropy, such that it failed to learn the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9aba1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long) # B, T = 1,1 holding zeros\n",
    "# 0 is standing as the new line character -> feed in as the first character\n",
    "\n",
    "# generates 100 new tokens from the model, starting with an initial context of a single token (the newline character, represented by 0).\n",
    "# The model's generate method returns a tensor of token indices, which is then converted to a list.\n",
    "# The decode function translates these token indices back into readable text, which is printed out.\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) \n",
    "\n",
    "# the code generated doesnt make sense because: \n",
    "# assuming we want to predict the character at position t, we only need the character at position t-1\n",
    "# but we feed in the whole context of the text so far, and use the last character as the input\n",
    "# history is not used rn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "33a1c082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToken embedding is a technique used in natural language processing to convert discrete tokens (such as words or characters) into continuous vector representations. Each unique token in the vocabulary is assigned a vector of real numbers, typically of fixed size. These vectors are learned during the training process and capture semantic or syntactic properties of the tokens.\\nIn the context of neural networks, especially transformer-based models, token embeddings serve as the initial input to the model. Instead of processing raw token indices, the model works with their corresponding embeddings, which allows it to learn relationships and patterns in the data more effectively.\\nFor example, in PyTorch, `nn.Embedding(vocab_size, embedding_dim)` creates a lookup table where each token index maps to an embedding vector of size `embedding_dim`. In the BigramLanguageModel above, the embedding table is set up so that each token is mapped directly to a vector of size equal to the vocabulary size, which is then used to predict the next token.\\n'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Token embedding is a technique used in natural language processing to convert discrete tokens (such as words or characters) into continuous vector representations. Each unique token in the vocabulary is assigned a vector of real numbers, typically of fixed size. These vectors are learned during the training process and capture semantic or syntactic properties of the tokens.\n",
    "In the context of neural networks, especially transformer-based models, token embeddings serve as the initial input to the model. Instead of processing raw token indices, the model works with their corresponding embeddings, which allows it to learn relationships and patterns in the data more effectively.\n",
    "For example, in PyTorch, `nn.Embedding(vocab_size, embedding_dim)` creates a lookup table where each token index maps to an embedding vector of size `embedding_dim`. In the BigramLanguageModel above, the embedding table is set up so that each token is mapped directly to a vector of size equal to the vocabulary size, which is then used to predict the next token.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "57b63c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimization object\n",
    "# simpler model torch.optim.SGD(m.parameters(), lr=1e-3)\n",
    "# typical learnign rate: 3e-4, 1e-3, 3e-2\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a0bc980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "# This section of code trains the language model using mini-batch gradient descent.\n",
    "# It sets the batch size, then runs a training loop for 10,000 steps.\n",
    "# In each step:\n",
    "#   - It samples a batch of training data (input and target sequences).\n",
    "#   - It computes the model's predictions and the loss.\n",
    "#   - It clears previous gradients, performs backpropagation to compute new gradients, and updates the model's parameters using the optimizer.\n",
    "# After training, it prints the final loss value.\n",
    "\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # backpropagate the loss\n",
    "    # zero_grad means that the gradients are set to zero, and set_to_none=True means that the gradients are not stored\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d1937248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))\n",
    "# doing the same thing as before but now the model is trained, if we sample from it again, it will generate coherent text (not best pereformance, but a lot better)\n",
    "# the model is not communicating with the rest of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f8020",
   "metadata": {},
   "source": [
    "The mathematical trick in self-attention:` we want different information talk to each other in the sentence. `\n",
    "Self-attention is a mechanism that allows each position in a sequence to attend to all positions in the same sequen ce, including itself. It's a key component of transformer architectures that enables the model to capture long-range dependencies and relationships between different parts of the input.\n",
    "\n",
    "The core idea is that for each position in the sequence, the model computes attention weights that determine how much focus to place on each other position when processing that particular position. This is done through three learned linear transformations:\n",
    "\n",
    "1. **Query (Q)**: Represents \"what am I looking for?\"\n",
    "2. **Key (K)**: Represents \"what do I have to offer?\"  \n",
    "3. **Value (V)**: Represents \"the actual content/information\"\n",
    "\n",
    "The attention mechanism computes a weighted sum of values, where the weights are determined by the compatibility between queries and keys. The mathematical formula is:\n",
    "\n",
    "**Attention(Q,K,V) = softmax(QK^T/√d_k)V**\n",
    "\n",
    "Where:\n",
    "- QK^T computes the similarity between queries and keys\n",
    "- The scaling factor √d_k prevents the dot products from becoming too large\n",
    "- softmax normalizes the attention weights to sum to 1\n",
    "- The final result is a weighted combination of the values\n",
    "\n",
    "This allows the model to dynamically focus on different parts of the input sequence when processing each position, making it particularly effective for tasks involving sequential data like language modeling.\n",
    "\n",
    "Note: \n",
    "We do not want the nth token to talk to n+1 token or the future tokens, because they are irrelevant to the current character. \n",
    "Current context should only communicated with previous context, not later. -> `Communicate with the past`\n",
    "\n",
    "Approach: we take the average of all the preceding elements: \n",
    "For example, if we are at the 5th position in the sequence, we want to compute attention using only the information from positions 1, 2, 3, 4, and 5 (the current position and all previous positions). We do NOT use information from positions 6, 7, 8, etc. because those are \"future\" tokens that haven't been generated yet during training. This creates a causal mask where each position can only attend to itself and all positions that came before it in the sequence.\n",
    "\n",
    "- Creates a feature vector that summarizes me in the context of my histroy. \n",
    "- Yet this form of communication is extremely lossy, the interaction between characters are weak, and we lose a lot of information about the spatial arrangements of all those tokens. \n",
    "- We will solve that in late step. \n",
    "\n",
    "- For every single element independently, for every teeth token in that sequence, we would like to calculate the average of all the vectors in all the previous tokena and all previous tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "df388ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # Batch, time, channel\n",
    "x = torch.randn(B,T,C) # have information at each point of the sequence\n",
    "print(x.shape)\n",
    "# print(x)\n",
    "\n",
    "# this is a 4x8 tensor, each element is either 0 or 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb95dc",
   "metadata": {},
   "source": [
    "Example of a (B, T, C) tensor where:\n",
    " - **B = 3** (batch size - 3 different sequences)\n",
    " - **T = 5** (time steps - 5 tokens in each sequence) \n",
    " - **C = 4** (channels/features - 4-dimensional embedding for each token)\n",
    " \n",
    " ```python\n",
    " torch.manual_seed(42)  # for reproducible results\n",
    " example_tensor = torch.randn(3, 5, 4)\n",
    " print(\"Shape:\", example_tensor.shape)\n",
    " print(\"Tensor:\")\n",
    " print(example_tensor)\n",
    " ```\n",
    " \n",
    " **Interpretation:**\n",
    " - **Batch 0** (first sequence): `example_tensor[0]` → 5×4 matrix (5 tokens, each with 4 features)\n",
    " - **Batch 1** (second sequence): `example_tensor[1]` → 5×4 matrix (5 tokens, each with 4 features)  \n",
    " - **Batch 2** (third sequence): `example_tensor[2]` → 5×4 matrix (5 tokens, each with 4 features)\n",
    " \n",
    " Each element `example_tensor[b, t, c]` represents:\n",
    " - **b**: which sequence in the batch (0, 1, or 2)\n",
    " - **t**: which position in the sequence (0, 1, 2, 3, or 4)\n",
    " - **c**: which feature dimension (0, 1, 2, or 3)\n",
    " \n",
    " **Example:** `example_tensor[1, 3, 2]` is the 3rd feature of the 4th token in the 2nd sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b6356631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    }
   ],
   "source": [
    "# version 1 using basic algebra\n",
    "\n",
    "# we want x[b,t] = mean_{i<=t} x[b,i]\n",
    "# xbow = x bag of words when we are avergaeing up things\n",
    "# Create a tensor to store the bag-of-words representation\n",
    "# Each position will contain the average of all previous tokens (including current)\n",
    "xbow = torch.zeros((B,T,C)) \n",
    "\n",
    "# For each batch element\n",
    "for b in range(B):\n",
    "    # For each time step in the sequence\n",
    "    for t in range(T):\n",
    "        # Calculate the mean of all tokens from start up to and including current position\n",
    "        # x[b,:t+1] gets all tokens from index 0 to t (inclusive)\n",
    "        # .mean(dim=0) averages across the time dimension, giving us a C-dimensional vector\n",
    "        xprev = x[b,:t+1]   #  (t,c) all the teeth tokens, with c as the 2D information from little tokens\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0) # dimension = 0 means average across the time dimension (first axis)\n",
    "      \n",
    "\n",
    "print(xbow.shape)\n",
    "print(xbow)\n",
    "\n",
    "# we hv established the correct form of xbow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "cc46a1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1032204a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79739112",
   "metadata": {},
   "source": [
    "The first tensor shows the original token embeddings for batch 0:\n",
    "Each row represents a token at position t, with 2 feature dimensions\n",
    "- Position 0: [0.1808, -0.0700] - first token's embedding\n",
    "- Position 1: [-0.3596, -0.9152] - second token's embedding  \n",
    "- Position 2: [0.6258, 0.0255] - third token's embedding\n",
    "- And so on for all 8 tokens in the sequence\n",
    "\n",
    "The second tensor shows the bag-of-words (averaged) representation:\n",
    "Each row now contains the running average of all previous tokens (including current)\n",
    "- Position 0: [0.1808, -0.0700] - same as original (only 1 token to average)\n",
    "- Position 1: [-0.0894, -0.4926] - average of tokens 0 and 1\n",
    "- Position 2: [0.1490, -0.3199] - average of tokens 0, 1, and 2\n",
    "- Position 3: [0.3504, -0.2238] - average of tokens 0, 1, 2, and 3\n",
    "- And so on...\n",
    "\n",
    "Notice how the values become more \"smoothed\" as we average more tokens,\n",
    "creating a cumulative representation that captures the context up to each position,\n",
    "but the idea is too inefficient when it comes to performing a huge set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0d96f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "print(a)\n",
    "print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "08249aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# we want the lower part to be all ones \n",
    "lower_triangular_mask = torch.tril(torch.ones(3,3))\n",
    "\n",
    "# we also want the upper part to be all ones \n",
    "upper_triangular_mask = torch.triu(torch.ones(3,3))\n",
    "\n",
    "print(lower_triangular_mask)\n",
    "print(upper_triangular_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7e2369b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# This demonstrates how to compute cumulative sums using matrix multiplication\n",
    "# The lower triangular matrix acts as a \"mask\" that sums all previous elements\n",
    "torch.manual_seed(42)\n",
    "d = torch.tril(torch.ones(3,3))  # Lower triangular matrix of ones\n",
    "e = torch.randint(0,10,(3,2)).float()  # Random data matrix\n",
    "f = d @ e  # Matrix multiplication computes cumulative sums\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b01a850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# to get the mean: \n",
    "d = d/ torch.sum(d, dim=1, keepdim=True)\n",
    "# dim = 1 means we're summing along dimension 1 (columns), so for each row we sum all the values in that row\n",
    "# This creates a column vector where each element is the sum of the corresponding row\n",
    "# keepdim=True preserves the original shape, so we get a 3x1 matrix instead of a 1D tensor\n",
    "print(d)\n",
    "f = d @ e \n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0a27db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# then we can do a matrix multiplication, note that the . means dot product \n",
    "c = a @ b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8a1cd439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2, using tril \n",
    "\n",
    "weight = torch.tril(torch.ones(T,T))\n",
    "wei = weight / weight.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4064b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x # this is the bag of words, but we want to do it in a way that the information cannot flow from the future to the past \n",
    "# This matrix multiplication performs weighted aggregation across the time dimension\n",
    "# (B,T,T) represents attention weights where each row sums to 1 (normalized)\n",
    "# (B,T,C) represents the input embeddings for each token at each time step\n",
    "# The result (B,T,C) gives each token a weighted combination of all previous tokens\n",
    "# \n",
    "# For each batch B and time step T, we compute:\n",
    "# output[b,t,c] = sum(weight[b,t,k] * input[b,k,c] for k in range(t+1))\n",
    "# This ensures information only flows from past to present (causal attention)\n",
    "torch.allclose(xbow, xbow2) # check if they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e670475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], \n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "98c6b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3 using softmax \n",
    "triangular_mask = torch.tril(torch.ones(T,T))\n",
    "weight_softmax = torch.zeros((T,T)) # in future, they are going to be data dependent and not zero\n",
    "# some tokens may be more interested in the future tokens rather than the past tokens, depending on the values, they are going to find different affinities of the data\n",
    "weight_softmax = weight_softmax.masked_fill(triangular_mask == 0, float('-inf'))\n",
    "# setting them to -inf means that the softmax will be 0 for those positions, they are not allowed to talk to each other/ previous tokens\n",
    "weight_softmax = F.softmax(weight_softmax, dim=-1) # normalization across the row\n",
    "weight_softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "49acc271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = weight_softmax @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a1e7141b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6813197",
   "metadata": {},
   "source": [
    "Self-Attention Mechanism: Query, Key, and Value Vectors\n",
    "\n",
    "- In self-attention, each token embedding produces three distinct vectors:\n",
    "\n",
    "- Query Vector (Q): Represents \"what am I looking for?\" - this vector encodes\n",
    "the information that the current token is seeking from other positions in the sequence.\n",
    "\n",
    "- Key Vector (K): Represents \"what do I contain?\" - this vector encodes the \n",
    "information that each token has available to share with other tokens.\n",
    "\n",
    "- Value Vector (V): Contains the actual information/content that gets passed\n",
    "to other tokens based on the attention weights.\n",
    "\n",
    "- Attention Score Calculation:\n",
    "To determine how much attention each token should pay to others, we compute\n",
    "the affinity between queries and keys using dot product similarity:\n",
    "Attention_Score = Q · K^T\n",
    "\n",
    "- The dot product measures alignment between vectors - when a query vector\n",
    "is well-aligned with a key vector (pointing in similar directions), their\n",
    "dot product will be large, indicating high affinity. Conversely, misaligned\n",
    "vectors will have small or negative dot products.\n",
    "\n",
    "learn data-dependent attention patterns where\n",
    "tokens can selectively focus on the most relevant information from other\n",
    "positions in the sequence, rather than treating all previous tokens equally.\n",
    "\n",
    "> A single head refers to one attention mechanism that learns a specific pattern of how tokens should attend to each other. Each head can focus on different types of relationships (e.g., syntactic, semantic, positional) and creates its own unique attention weights. Multiple heads allow the model to capture diverse attention patterns simultaneously.\n",
    "\n",
    "\n",
    "\n",
    "- Bias is an additional learnable parameter in neural network layers that allows the model to shift the activation function. It's essentially a constant value that gets added to the weighted sum of inputs before applying the activation function.\n",
    "```\n",
    "output = activation(weighted_sum + bias)\n",
    "where weighted_sum = input1 * weight1 + input2 * weight2 + ... + inputN * weightN\n",
    "```\n",
    "\n",
    "Why bias is important:\n",
    "- **Flexibility**: Without bias, the activation function can only pass through the origin (0,0). Bias allows the function to be shifted horizontally, giving the model more flexibility to fit the data.\n",
    "- **Learning capability**: It enables the model to learn patterns that don't pass through the origin, which is crucial for most real-world problems.\n",
    "- **Decision boundary**: In classification tasks, bias helps position the decision boundary optimally in the feature space.\n",
    "\n",
    "In the context of the linear layer `nn.Linear(C, head_size, bias=False)`:\n",
    "- Setting `bias=False` means the layer will NOT learn a bias term\n",
    "- This is sometimes done to reduce parameters or when you want the layer to be purely linear without any offset\n",
    "- For attention mechanisms, bias is often omitted because the attention weights are normalized anyway\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "35454ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4 self attention block \n",
    "from math import inf\n",
    "# Set random seed for reproducible results\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Define dimensions: Batch size=4, Time steps=8, Channels=32\n",
    "B,T,C = 4,8,32 \n",
    "# Create random input tensor representing token embeddings\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Create lower triangular matrix (1s below diagonal, 0s above)\n",
    "# This ensures causal attention - tokens can only attend to previous positions\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "# Initialize attention weights as zeros, but later we will modify the weighting, we do not want uniform weighting, since some of the chracters might be of higher importance, we want the thing we are looking for to flow to us. \n",
    "# we want to gather information from the past but we want to do it in the data dependent way. \n",
    "# single head self attention \n",
    "\n",
    "\n",
    "# no communicarion is happening here, we are just projecting the data into a smaller space, we are not attending to anything yet \n",
    "\n",
    "head_size = 16 # the number of dimensions in the query, key, and value vectors\n",
    "key = nn.Linear(C, head_size, bias = False) \n",
    "query = nn.Linear(C, head_size, bias = False) \n",
    "k = key(x)   # (B,T,head_size)\n",
    "q = query(x) # (B,T,head_size)\n",
    "\n",
    "# communication is happening here, dot product:\n",
    "wei = q @ k.transpose(-2, -1) # (B,T,T) -> why transpose? because we want to do a dot product between the query and the key, and we want to do it in the time dimension, not the channel dimension\n",
    "# we want (B,T,16) @ (B,16,T) -> (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "\n",
    "out = wei @ x\n",
    "\n",
    "# Output shape is (B,T,C) - same as input but with attention applied\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "dde0a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0] # raw affinities between all the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02876e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wei = torch.zeros((T,T))\n",
    "\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float(-inf)) # not allowed to communicated\n",
    "# Apply softmax to normalize attention weights across each row\n",
    "# Each row now sums to 1, representing attention distribution\n",
    "wei = F.softmax(wei, dim = -1)\n",
    "# dim = -1 because we want to apply softmax across the last dimension (the time dimension T)\n",
    "# This ensures that for each query position, the attention weights sum to 1 across all key positions\n",
    "# Each row in the attention matrix represents attention distribution for one query position\n",
    "# Apply attention weights to input embeddings\n",
    "# This creates weighted combinations where each token aggregates information\n",
    "# from all previous tokens (including itself) based on attention scores\n",
    "\n",
    "out = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d0869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]\n",
    "# data dependent matter and how much of data is useful to aggregrate from any of these tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97210c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e9be31",
   "metadata": {},
   "source": [
    " \n",
    "- When a key vector has a high value in a specific channel (dimension), and a query vector also has a high value in the same channel, their dot product will result in a high affinity score\n",
    "- This high affinity indicates that these two positions share similar patterns or features in that particular dimension\n",
    "- Through the softmax normalization, this high affinity gets amplified and converted into a large attention weight\n",
    "- This allows the current position to learn and incorporate information from the previous position that has similar characteristics\n",
    "- For example, if we're at position 8 looking for consonants, and position 3 has a key vector with high values in the consonant-related channels, their dot product will be large\n",
    "- After softmax, position 8 will pay strong attention to position 3, effectively learning consonant patterns from the previous context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6289f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei \n",
    "# The attention weights are now data-dependent, considering both the position and content of tokens\n",
    "# Each row represents how much attention each position pays to all previous positions (including itself)\n",
    "# The weights are learned through the query-key dot product and normalized via softmax \n",
    "\n",
    "# e.g. i am in wei[0][0][0][8], i am at position 8 and contains the vowel, i am looking for some constants from 0 - 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "abf7188c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fc6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7facbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
